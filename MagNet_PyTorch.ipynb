{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to convert string representations using Mousavi's method\n",
    "def string_convertor(dd):\n",
    "    dd2 = dd.split()\n",
    "    SNR = []\n",
    "    for i, d in enumerate(dd2):\n",
    "        if d != '[' and d != ']':\n",
    "            dL = d.split('[')\n",
    "            dR = d.split(']')\n",
    "            if len(dL) == 2:\n",
    "                dig = dL[1]\n",
    "            elif len(dR) == 2:\n",
    "                dig = dR[0]\n",
    "            elif len(dR) == 1 and len(dR) == 1:\n",
    "                dig = d\n",
    "            try:\n",
    "                dig = float(dig)\n",
    "            except Exception:\n",
    "                dig = None\n",
    "            SNR.append(dig)\n",
    "    return SNR\n",
    "\n",
    "# Dataset class\n",
    "class EarthquakeDataset(Dataset):\n",
    "    def __init__(self, file_name, file_list):\n",
    "        self.file_name = file_name\n",
    "        self.file_list = file_list\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        self.metadata = {\n",
    "            'net_code': [], 'rec_type': [], 'eq_id': [], 'eq_depth': [], 'eq_mag': [],\n",
    "            'mag_type': [], 'mag_auth': [], 'eq_dist': [], 'snr': [], 'trace_name': [],\n",
    "            'S_P': [], 'baz': []\n",
    "        }\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        with h5py.File(self.file_name, 'r') as dtfl:\n",
    "            for evi in tqdm(self.file_list):\n",
    "                dataset = dtfl.get(f'earthquake/local/{evi}')\n",
    "                if dataset is None:\n",
    "                    print(f\"Dataset not found for event ID: {evi}\")\n",
    "                    continue\n",
    "                data = np.array(dataset)\n",
    "                spt = int(dataset.attrs['p_arrival_sample'])\n",
    "                if spt < 100 or spt + 2900 > len(data):\n",
    "                    print(f\"Invalid sample range for event ID: {evi}\")\n",
    "                    continue\n",
    "                dshort = data[spt-100:spt+2900, :]\n",
    "                self.X.append(dshort)\n",
    "                mag = round(float(dataset.attrs['source_magnitude']), 2)\n",
    "                self.Y.append(mag)\n",
    "\n",
    "                # Extract additional metadata\n",
    "                self.metadata['net_code'].append(dataset.attrs['network_code'])\n",
    "                self.metadata['rec_type'].append(dataset.attrs['receiver_type'])\n",
    "                self.metadata['eq_id'].append(dataset.attrs['source_id'])\n",
    "                self.metadata['eq_depth'].append(dataset.attrs['source_depth_km'])\n",
    "                self.metadata['eq_mag'].append(mag)\n",
    "                self.metadata['mag_type'].append(dataset.attrs['source_magnitude_type'])\n",
    "                self.metadata['mag_auth'].append(dataset.attrs['source_magnitude_author'])\n",
    "                self.metadata['eq_dist'].append(round(float(dataset.attrs['source_distance_deg']), 2))\n",
    "                self.metadata['snr'].append(round(np.mean(dataset.attrs['snr_db']), 2))\n",
    "                self.metadata['trace_name'].append(dataset.attrs['trace_name'])\n",
    "                self.metadata['S_P'].append(int(dataset.attrs['s_arrival_sample']) - spt)\n",
    "                self.metadata['baz'].append(round(float(dataset.attrs['back_azimuth_deg']), 2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.Y[idx], dtype=torch.float32),\n",
    "            self.metadata['net_code'][idx],\n",
    "            self.metadata['rec_type'][idx],\n",
    "            self.metadata['eq_id'][idx],\n",
    "            self.metadata['eq_depth'][idx],\n",
    "            self.metadata['eq_mag'][idx],\n",
    "            self.metadata['mag_type'][idx],\n",
    "            self.metadata['mag_auth'][idx],\n",
    "            self.metadata['eq_dist'][idx],\n",
    "            self.metadata['snr'][idx],\n",
    "            self.metadata['trace_name'][idx],\n",
    "            self.metadata['S_P'][idx],\n",
    "            self.metadata['baz'][idx]\n",
    "        )\n",
    "\n",
    "\n",
    "# Define the paths to your data files\n",
    "file_name = \"/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/STEAD/merge.hdf5\"\n",
    "csv_file = \"/content/drive/My Drive/2023-2024/UCL MSc in DSML/Term 3/MSc Project/Code/Datasets/STEAD/merge.csv\"\n",
    "\n",
    "# Verify file existence\n",
    "assert os.path.isfile(file_name), f\"HDF5 file not found at {file_name}\"\n",
    "assert os.path.isfile(csv_file), f\"CSV file not found at {csv_file}\"\n",
    "\n",
    "# Load and filter the dataset\n",
    "df = pd.read_csv(csv_file, low_memory=False)\n",
    "print(f\"Initial number of records: {len(df)}\")\n",
    "\n",
    "# Filtering the dataset\n",
    "df = df[df.trace_category == 'earthquake_local']\n",
    "df = df[df.source_distance_km <= 110]\n",
    "df = df[df.source_magnitude_type == 'ml']\n",
    "df = df[df.p_arrival_sample >= 200]\n",
    "df = df[df.p_arrival_sample + 2900 <= 6000]\n",
    "df = df[df.p_arrival_sample <= 1500]\n",
    "df = df[df.s_arrival_sample >= 200]\n",
    "df = df[df.s_arrival_sample <= 2500]\n",
    "\n",
    "# Fix coda_end_sample column parsing\n",
    "df['coda_end_sample'] = df['coda_end_sample'].apply(lambda x: float(x.strip('[]')))\n",
    "df = df.dropna(subset=['coda_end_sample'])\n",
    "df = df[df['coda_end_sample'] <= 3000]\n",
    "\n",
    "df = df[df.p_travel_sec.notnull()]\n",
    "df = df[df.p_travel_sec > 0]\n",
    "df = df[df.source_distance_km.notnull()]\n",
    "df = df[df.source_distance_km > 0]\n",
    "df = df[df.source_depth_km.notnull()]\n",
    "df = df[df.source_magnitude.notnull()]\n",
    "df = df[df.back_azimuth_deg.notnull()]\n",
    "df = df[df.back_azimuth_deg > 0]\n",
    "df['snr_db'] = df['snr_db'].apply(lambda x: np.mean(string_convertor(x)))\n",
    "df = df[df.snr_db >= 20]\n",
    "\n",
    "# Implementing multi-observations with a threshold of >= 400\n",
    "uniq_ins = df.receiver_code.unique()\n",
    "\n",
    "labM = []\n",
    "for ii in range(0, len(uniq_ins)):\n",
    "    print(str(uniq_ins[ii]), sum(n == str(uniq_ins[ii]) for n in df.receiver_code))\n",
    "    stn = sum(n == str(uniq_ins[ii]) for n in df.receiver_code)\n",
    "    if stn >= 400:\n",
    "        labM.append(str(uniq_ins[ii]))\n",
    "\n",
    "np.save('multi_observations', labM)\n",
    "\n",
    "multi_observations = np.load('multi_observations.npy')\n",
    "ev_list = []\n",
    "for index, row in df.iterrows():\n",
    "    st = row['receiver_code']\n",
    "    if st in multi_observations:\n",
    "        ev_list.append(row['trace_name'])\n",
    "\n",
    "ev_list = df.trace_name.tolist()\n",
    "np.random.shuffle(ev_list)  \n",
    "\n",
    "# Splitting data into training, validation, and test sets\n",
    "training = ev_list[:int(0.7*len(ev_list))]\n",
    "validation = ev_list[int(0.7*len(ev_list)): int(0.8*len(ev_list))]\n",
    "test = ev_list[int(0.8*len(ev_list)):]\n",
    "\n",
    "# Creating datasets and data loaders\n",
    "train_dataset = EarthquakeDataset(file_name, training)\n",
    "val_dataset = EarthquakeDataset(file_name, validation)\n",
    "test_dataset = EarthquakeDataset(file_name, test)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the PyTorch model\n",
    "class EarthquakeModel(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(EarthquakeModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3, padding='same')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.pool1 = nn.MaxPool1d(4, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding='same')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.pool2 = nn.MaxPool1d(4, padding=1)\n",
    "\n",
    "        self.lstm = nn.LSTM(32, 100, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(200, 2)  # 2 for mean and variance\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change the shape from (batch, seq, features) to (batch, features, seq)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Change the shape back to (batch, seq, features) for LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = self.fc(x[:, -1, :])  # Use the output of the last time step\n",
    "        return x\n",
    "\n",
    "# Custom loss function for aleatoric uncertainty\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_hat = y_pred[:, 0]\n",
    "    s = y_pred[:, 1]\n",
    "    return torch.mean(0.5 * torch.exp(-s) * (y_true - y_hat) ** 2 + 0.5 * s)\n",
    "\n",
    "# Create the model\n",
    "drop_rate = 0.2\n",
    "model = EarthquakeModel(dropout_rate=drop_rate)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=4, min_lr=0.5e-6)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target, *_) in enumerate(train_loader):  # ignore metadata\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = custom_loss(target, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f'Train Epoch: {epoch} \\tLoss: {train_loss:.6f}')\n",
    "    return train_loss\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, *_) in val_loader:  # ignore metadata\n",
    "            output = model(data)\n",
    "            val_loss += custom_loss(target, output).item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f'Validation set: Average loss: {val_loss:.6f}\\n')\n",
    "    return val_loss\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, epoch)\n",
    "    val_loss = validate(model, val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save model if validation loss decreases\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Define the MCDropoutModel for uncertainty estimation\n",
    "class MCDropoutModel(nn.Module):\n",
    "    def __init__(self, model, n_iter=50):\n",
    "        super(MCDropoutModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model.train()\n",
    "        preds = []\n",
    "        for _ in range(self.n_iter):\n",
    "            preds.append(self.model(x))\n",
    "        preds = torch.stack(preds)\n",
    "        return preds\n",
    "\n",
    "# Function to compute uncertainties\n",
    "def compute_uncertainties(preds):\n",
    "    pred_mean = preds[:, :, 0].mean(dim=0)\n",
    "    pred_var = preds[:, :, 0].var(dim=0)\n",
    "    aleatoric_unc = preds[:, :, 1].mean(dim=0)\n",
    "    epistemic_unc = pred_var\n",
    "    combined_unc = aleatoric_unc + epistemic_unc\n",
    "    return pred_mean, aleatoric_unc, epistemic_unc, combined_unc\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Create MCDropoutModel\n",
    "mc_model = MCDropoutModel(model)\n",
    "\n",
    "# Test the model and compute uncertainties\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "test_metadata = {key: [] for key in train_dataset.metadata.keys()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target, *metadata in test_loader:\n",
    "        preds = mc_model(data)\n",
    "        test_preds.append(preds)\n",
    "        test_targets.append(target)\n",
    "        \n",
    "        for key, value in zip(test_metadata.keys(), metadata):\n",
    "            test_metadata[key].append(value)\n",
    "\n",
    "test_preds = torch.cat(test_preds, dim=1)\n",
    "test_targets = torch.cat(test_targets)\n",
    "\n",
    "pred_mean, aleatoric_unc, epistemic_unc, combined_unc = compute_uncertainties(test_preds)\n",
    "\n",
    "# Plotting and saving results\n",
    "# Plotting training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.grid()\n",
    "plt.savefig('loss_curve.png')\n",
    "\n",
    "# Plotting uncertainties\n",
    "fig1 = plt.figure()\n",
    "plt.errorbar(pred_mean, aleatoric_unc, xerr=aleatoric_unc, fmt='o', alpha=0.4, ecolor='g', capthick=2)\n",
    "plt.plot(test_targets, aleatoric_unc, 'ro', alpha=0.4)\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Aleatoric Uncertainty')\n",
    "plt.title('Aleatoric Uncertainty')\n",
    "fig1.savefig('aleatoric_uncertainty.png')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.errorbar(pred_mean, epistemic_unc, xerr=epistemic_unc, fmt='o', alpha=0.4, ecolor='g', capthick=2)\n",
    "plt.plot(test_targets, epistemic_unc, 'ro', alpha=0.4)\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Epistemic Uncertainty')\n",
    "plt.title('Epistemic Uncertainty')\n",
    "fig2.savefig('epistemic_uncertainty.png')\n",
    "\n",
    "fig3 = plt.figure()\n",
    "plt.errorbar(pred_mean, combined_unc, xerr=combined_unc, fmt='o', alpha=0.4, ecolor='g', capthick=2)\n",
    "plt.plot(test_targets, combined_unc, 'ro', alpha=0.4)\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Combined Uncertainty')\n",
    "plt.title('Combined Uncertainty')\n",
    "fig3.savefig('combined_uncertainty.png')\n",
    "\n",
    "fig4, ax = plt.subplots()\n",
    "ax.scatter(test_targets, pred_mean, alpha=0.4, facecolors='none', edgecolors='r')\n",
    "ax.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'k--', alpha=0.4, lw=2)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "fig4.savefig('measured_vs_predicted.png')\n",
    "\n",
    "# Save additional metadata\n",
    "for key in test_metadata.keys():\n",
    "    test_metadata[key] = [item for sublist in test_metadata[key] for item in sublist]\n",
    "\n",
    "# Saving results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'true_magnitude': test_targets.numpy(),\n",
    "    'predicted_magnitude': pred_mean.numpy(),\n",
    "    'aleatoric_uncertainty': aleatoric_unc.numpy(),\n",
    "    'epistemic_uncertainty': epistemic_unc.numpy(),\n",
    "    'combined_uncertainty': combined_unc.numpy(),\n",
    "    'net_code': test_metadata['net_code'],\n",
    "    'rec_type': test_metadata['rec_type'],\n",
    "    'eq_id': test_metadata['eq_id'],\n",
    "    'eq_depth': test_metadata['eq_depth'],\n",
    "    'eq_mag': test_metadata['eq_mag'],\n",
    "    'mag_type': test_metadata['mag_type'],\n",
    "    'mag_auth': test_metadata['mag_auth'],\n",
    "    'eq_dist': test_metadata['eq_dist'],\n",
    "    'snr': test_metadata['snr'],\n",
    "    'trace_name': test_metadata['trace_name'],\n",
    "    'S_P': test_metadata['S_P'],\n",
    "    'baz': test_metadata['baz']\n",
    "})\n",
    "results_df.to_csv('results.csv', index=False)\n",
    "\n",
    "print('Results have been saved successfully.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
